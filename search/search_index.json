{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#generation-of-a-virtual-machine-vm-from-an-ubuntu-server-installation-image-iso-file","title":"Generation of a virtual machine (VM) from an ubuntu server installation image (ISO file)","text":"<ul> <li>containing The Littlest JupyerHub (TLJH)</li> <li>with the help of Packer, Vagrant and Ansible tools</li> <li>for using with VirtualBox or OpenStack.</li> </ul> <p>All the tools used are exclusively free and open-source.</p> <p>The Littlest JupyterHub, a recent and evolving distribution designed for smaller deployments, is a lightweight method to install JupyterHub on a single virtual machine. The Littlest JupyterHub (also known as TLJH), provides a guide with information on creating a VM on several cloud providers, as well as installing and customizing JupyterHub so that users may access it at a public URL.</p>"},{"location":"#overview","title":"Overview","text":"<p>This work was carried on a desktop computer running Windows10 / Cygwin. But any machine (laptop, desktop, server, ...) under MacOS or UNIX-like can be perfectly suitable. Please note that this requires technical skills in installing applications and assumes that you are comfortable editing configuration files. </p> <p>Note: All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository.</p> <p>To build our final VM, i.e. the VM that will be instantiated for use locally (type 2 virtualization) or on a remote server (Datacenter or Cloud), we will proceed in two main steps for the creation. The first step will consist in the building of a base VM (Base box). The second step will consist, from the base VM, in installing and configuring all the system and application tools (packages) necessary to make JupyterHub work correctly from a web browser. A third step will describe how to instantiate the final VM on the Genouest cloud. The figure below gives an overview: </p> <p></p> <p></p> <p>The three main stages</p> <ol> <li>Building the base VM (Base box) for Vagrant</li> <li>Building the final VM</li> <li>Using the final VM on an Openstack cloud</li> </ol> <p>Pipeline</p> <ul> <li>In green, the pipeline path for the creation, storage and instantiation of the virtual machine.</li> <li>As input to the pipeline, an ISO file corresponding to the chosen operating system and downloaded from the Internet. </li> <li>At the output, an instance of the virtual machine operational on Genouest's Openstack cloud.</li> </ul> <p>The different layers</p> <ul> <li>The configuration layer corresponds to all the files and scripts deposited in the github.</li> <li>The creation layer corresponds to all the tools installed and used on the user's local machine (except for Ansible which is installed on the virtual machine but which could very well be installed on the local machine).</li> <li>The storage layer corresponds to the storage sites of the VMs (base and final).</li> <li>The instantiation layer corresponds to the instantiated virtual machine.</li> </ul> <p></p>"},{"location":"basebox/","title":"Base Box","text":""},{"location":"basebox/#building-the-base-vm-base-box-for-vagrant","title":"Building the base VM (Base box) for Vagrant","text":"<p>It is assumed that you have installed the Packer and Vagrant tools, as well as the Oracle VirtualBox virtualisation software.</p> <p>The operations for creating a base box are documented online; you have to follow the basic guide and apply the provider-specific rules, namely for VirtualBox . The procedure is somewhat tedious and, above all, an error can happen quickly. This is why we will use the Packer tool, which allows to fully automate the creation of a Vagrant base box by following the process illustrated by the figure below:</p> <p></p> <p></p> <p>Note: All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository.</p> <p>We are using Ubuntu version 20.04 as a basis.</p> <p>A Packer configuration is defined with a JSON file (box-config.json) with several sections: </p> <ul> <li>builders: is used to define the elements for creating the virtual machine from an ISO image.</li> <li>provisioners: is used to define the software configuration from Shell scripts to provision the virtual machine.</li> <li>post-processors: runs once the virtual machine is created and provisioned. This allows among other things to define the output format of the VM.</li> </ul> <p>\"builders\" section</p> <ul> <li>To build this section, it is often simpler to start from already functional examples and to modify the few elements specific to its configuration. A query in a search engine (e.g. Google) with the keywords \"github packer ubuntu 20.04 virtualbox\" will give you enough examples. So we started with an example appropriate to our needs, which we then adapted.</li> <li>The VirtualBox box is made from an Ubuntu ISO specified by its URL (iso_urls) in order to create the VM in VirtualBox (\"type\": \"virtualbox-iso\") .</li> <li>A \"preseed\" file (http/preseed.cfg) allows to configure the installation (see more info on preseeding ). We have adapted the example by adding instructions concerning the root account and the network configuration.</li> <li>We limited the maximum disk size to 18GB (\"disk_size\": 18432) so that the final VM can be accepted by the Genouest cloud.</li> </ul> <p>\"provisioners\" section</p> <p>This section allows to run Shell scripts after the virtual machine has booted properly and then its operating system installed. So it is in this step that we can configure the VM to be compatible with Vagrant and VirtualBox. A Shell script (scripts/setup.sh) is then executed in order to:</p> <ul> <li>install the drivers for VirtualBox</li> <li>configure SSH accesses in compliance with Vagrant boxes. </li> </ul> <p>\"post-processors\" section</p> <ul> <li>Once the base virtual machine is fully installed and configured, it is simply exported to Vagrant format. </li> </ul>"},{"location":"basebox/#creating-the-base-vm","title":"Creating the base VM","text":"<p>Before building, you must install these external plugins to ensure builds keep working </p> <pre><code>$&gt; packer plugins install github.com/hashicorp/virtualbox\n$&gt; packer plugins install github.com/hashicorp/vagrant\n</code></pre> <p>Once the configuration has been established, simply run Packer as follows:</p> <pre><code>$&gt; packer build box-config.json\n</code></pre> <p>All the messages produced can be consulted on the github repository. The base VM after execution can be found under the builds directory.</p> <p></p>"},{"location":"basebox/#registering-a-box-on-vagrant-cloud","title":"Registering a box on Vagrant Cloud","text":"<p>Vagrant Cloud provides an API that allows users to register their virtual machines (boxes) with Vagrant Cloud so that they can be reused by themselves or by other users.  The use of the API is described online. </p> <p>The registration of a virtual machine can only be done with the Vagrant format, i.e. a \"box\". A \"box\" is in fact a zipped archive file (TAR + GZIP) with the extension '.box'. Registering a \"box\" consists of 5 steps:</p> <ol> <li>Creating an entry for the \"box\": at least the name of the box (boxname) must be specified. Its description is optional.</li> <li>Creating a version: there may be several versions of the same entry; the version must be specified.</li> <li>Creation of a provider: similarly, for an entry (boxname) and a version, there may be several associated providers; the provider must be specified.</li> <li>Upload the box file.</li> <li>Validate the box. (entry + version + provider)</li> </ol> <p>All of these steps can be done either via the Vagrant Cloud web interface, or through multiple API calls. In order to facilitate the automation of registrations, the Vagrant tool provides a 'cloud' functionality that allows you to register your box on Vagrant Cloud. This requires an account (called an 'organisation') to be created on Vagrant Cloud. </p> <p>Example of invocation : </p> <p><pre><code>$&gt; vagrant cloud auth login\n</code></pre> <pre><code> ...\nVagrant Cloud username: GAEV\nVagrant Cloud password: XXXXXXX\n</code></pre></p> <p><pre><code>$&gt; vagrant cloud publish GAEV/centos7-dd8Gb 1.0.0 virtualbox virtualbox-centos7_8Gb.box \n</code></pre> <pre><code>You are about to create a box on Vagrant Cloud with the following options:\nGAEV/centos7-dd8Gb (1.0.0) for virtualbox\nAutomatic Release:     true\nDo you wish to continue? [y/N] y\nCreating a box entry...\nCreating a version entry...\nCreating a provider entry...\nUploading provider with file /Vagrant/boxes/virtualbox-centos7_8Gb.box\nReleasing box...\nComplete!\ntag:                  GAEV/centos7-dd8Gb\nusername:             GAEV\nname:                 centos7-dd8Gb\nprivate:              false\ndownloads:            0\ncreated_at:           2020-07-25T17:53:04.340Z\nupdated_at:           2020-07-25T18:01:10.665Z\ncurrent_version:      1.0.0\nproviders:            virtualbox\n</code></pre></p> <p>The registered box can be viewed on Vagrant Cloud.</p> <p></p>"},{"location":"finalvm/","title":"Final VM","text":""},{"location":"finalvm/#starting-from-an-existing-base-vm-base-box","title":"Starting from an existing base VM (Base box)","text":"<p>Creating a base VM can be complex for a non-expert. This is why it is easier to start from a base VM when it is available. This is also why the base VM should be as generic as possible so that it can be easily used for many applications.</p> <p>The base VM generated and used in this use-case was deposited on Vagrant Cloud, using its API (see Registering a box on Vagrant Cloud).</p>"},{"location":"finalvm/#building-the-final-vm","title":"Building the final VM","text":"<p>We will use the base VM to provision it, i.e. install and configure all the system and application tools (packages) necessary to make the JupyterHub application work properly. To do this, we will use the Ansible tool as illustrated in the figure below:</p> <p></p> <p></p> <p>Note: All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository.</p> <p>The first step is to create a Vagrantfile and customise it to suit your needs. This file follows the syntax of the Ruby language. A light version is given below:</p> <pre><code>  Vagrant.configure(\"2\") do |config|\n\n    config.vm.box = file://builds/virtualbox-ubuntu1804.box\n    config.vm.hostname = jupyterhub\n\n    config.vm.network \"private_network\", type: \"dhcp\"\n\n    config.ssh.insert_key = false\n\n    config.vm.synced_folder \".\", \"/vagrant\", type: \"virtualbox\"\n\n    config.vm.provision \"ansible_local\" do |ansible|\n        ansible.playbook = \"ansible/playbook.yml\"\n        ansible.install = true\n        ansible.limit = 'all'\n    end\n\n    config.vm.provision \"shell\", path: \"scripts/cleanup.sh\"\n\n  end\n</code></pre> <ul> <li>config.vm.box<ul> <li>sets the base box as input</li> </ul> </li> <li>config.vm.hostname<ul> <li>defines the hostname of the machine</li> </ul> </li> <li>config.vm.network<ul> <li>defines the network configuration in DHCP mode</li> </ul> </li> <li>config.ssh.insert_key<ul> <li>defines the SSH key. By default, it will be the one defined in the base box.</li> </ul> </li> <li>config.vm.synced_folder<ul> <li>defines a local directory as shared with the VM. At least, the \".\" directory must be defined in order to configure the VM. These shared directories will only be active during the creation and then test stage before export.</li> </ul> </li> <li>config.vm.provision<ul> <li>defines the process by which the VM will be provisioned. Here, two processes are invoked. i) 'ansible_local': indicates that the ansible tool will first be installed on the VM and then use the playbook defined in ansible/playbook; ii) 'shell': indicates that a shell script defined by its relative path will be used.</li> </ul> </li> </ul>"},{"location":"finalvm/#provisioning-using-the-ansible-tool","title":"Provisioning using the Ansible tool","text":"<p>The choice was made not to install Ansible on the host machine used to create the VM, but rather it will be installed on the VM itself, this is to simplify the process.</p> <p>The provisioning is defined from a YAML file called playbook.yml</p> <pre><code>  ---\n- hosts: all\n  become: true\n  become_user: root\n  #gather_facts: no\n\n  vars_files:\n    - vars/all.yml\n\n  environment:\n    PYTHONHTTPSVERIFY: 0\n\n  roles:\n    - repositories\n    - server\n    - vm\n    - install-r\n    - jupyterhub\n    - r_pkgs\n    - python_pkgs\n</code></pre> <p>Ansible allows tasks to be organised in a directory structure called a Role. In this configuration, playbooks invoke roles, which themselves define a set of tasks, so you can always group tasks together and reuse roles in other playbooks. Roles also allow you to collect templates, static files and variables along with your tasks in a structured format. </p> <p>Each role, and then each task within roles, is interpreted sequentially. Thus the following roles are defined:</p> <ul> <li>repositories<ul> <li>configures the repositories for binary packages (systems, R, python, tools, ...), </li> </ul> </li> <li>server<ul> <li>configure the timezone </li> </ul> </li> <li>vm<ul> <li>configure the hostname</li> </ul> </li> <li>install-r<ul> <li>installs the R application with basic packages</li> </ul> </li> <li>jupyterhub<ul> <li>installs and configures the jupyterhub application</li> </ul> </li> <li>r_pkgs<ul> <li>install a set of R packages</li> </ul> </li> <li>python_pkgs<ul> <li>install a set of Python packages</li> </ul> </li> </ul> <p>Each role is defined by a set of tasks, themselves defined by files in YAML format.</p> <p>For example, the role 'repositories' is defined by the following roles/repositories/tasks/main.yml file:</p> <pre><code>---\n\n- name: Add an apt key by id from a keyserver\n  apt_key:\n    keyserver: \"{{repository.keyserver}}\"\n    id: \"{{repository.id}}\"\n\n- name: Add repositories\n  apt_repository:\n    repo: \"{{item}}\"\n  with_items: \"{{repository.repos}}\"\n</code></pre> <p>Two tasks are thus defined and described by their name field. Then the actual task is defined using a name referring to a module. Hundreds of modules are available in Ansible. Here two modules are invoked:</p> <ul> <li>apt_key: add or remove an apt key</li> <li>apt_repository: add or remove a binary package repository. The variables correspond to those defined in the }  defined by the double braces {{ }vars/all.yml file itself declared in the playbook. Thus the following lines can be found in this file:</li> </ul> <pre><code>repository:\n    keyserver: hkp://keyserver.ubuntu.com:80\n    id: E298A3A825C0D65DFD57CBB651716619E084DAB9\n    repos:\n      - ppa:c2d4u.team/c2d4u4.0+\n      - ppa:hnakamur/libgit2\n      - deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/\n</code></pre> <p>So this is how the provisioning task set is defined by Ansible. </p> <p>The 'jupyterhub' role was built from the instructions provided on the TLJH website. The list of Python and R packages needed for a specific use under JupyterHub are to be defined in the vars/all.yml file, whose installation is taken care of by the 'python_pkgs' and 'r_pkgs' roles respectively.</p>"},{"location":"finalvm/#shell-script-provisioning","title":"Shell script provisioning","text":"<p>Once the provisioning by Ansible is finished, we proceed to a VM cleanup, i.e. uninstall Ansible, clean the temporary files and clean the hard disk. We use a Shell script (scripts/cleanup.sh) for this.</p>"},{"location":"finalvm/#creating-the-final-vm","title":"Creating the final VM","text":"<p>Once the configuration is established, you just have to execute the following commands:</p> <pre><code>$&gt; vagrant up\n$&gt; vagrant package --output ubuntu-box.tar.gz\n</code></pre> <p>All the messages produced can be consulted on the github repository</p> <p></p>"},{"location":"google-cloud/","title":"Google Cloud","text":""},{"location":"google-cloud/#using-google-cloud-sdk","title":"Using Google Cloud SDK","text":""},{"location":"google-cloud/#before-you-begin","title":"Before you begin","text":"<p>In order to use the final VM on GCP, you first need to go through the following steps:</p> <ul> <li>Assuming your project is created (GCP), </li> <li>Assuming your SSH Keys defined at the level project (GCP)</li> <li>Assuming your SSH keys are available (added with ssh-add, a ssh-agent  running),</li> <li>Assuming the Google Cloud SDK installed on your machine (See How to install Google Cloud SDK)</li> </ul> <p></p>"},{"location":"google-cloud/#quick-overview","title":"Quick overview","text":"<p>We start from the VM final box and will then proceed in two steps:</p> <ol> <li>Upload the final box VM file (vmdk) to Cloud Storage and convert it to an image</li> <li>Create an instance of this image</li> </ol> <p></p> <p></p>"},{"location":"google-cloud/#create-an-image-from-the-vm-final-box","title":"Create an image from the VM final box","text":""},{"location":"google-cloud/#preparation","title":"Preparation","text":"<p>A number of variables need to be defined in order to set up the process correctly. The values indicated correspond to the test performed. Some of them must therefore be adapted to your context (OS, resources).</p> <p></p> <ul> <li>Define the full path to the google-cloud utility to simplify further commands. </li> </ul> <p><pre><code>GCLOUD=/cygdrive/c/_Tools/gcloud/google-cloud-sdk/bin/gcloud\n</code></pre> </p> <ul> <li>Initialization of variables concerning the project. You must adapt these values based on your own project</li> </ul> <pre><code>PROJECT=test-gaev-20210906\nZONE=europe-west1-b\n</code></pre> <p></p> <ul> <li>Initialization of variables concerning the VM. The path (VDISK_DIR) and the name of the vmdk file (VMDK_FILE) must be changed according to your case. The type of virtual machines (VM_MACHINE) can also be adapted according to your needs (see machine families)</li> </ul> <pre><code>IMAGE=jupyterhub-img\nVM=jupyterhub-vm\nVDISK_DIR=/cygdrive/c/VirtualMach/Vagrant/jupyterhub/builds/vm\nVMDK_FILE=box-disk001.vmdk\nVM_OS=ubuntu-1804\nVM_MACHINE=c2-standard-4\n</code></pre> <p></p> <ul> <li>Set the project by default</li> </ul> <pre><code>export CLOUDSDK_CORE_PROJECT=$PROJECT\n</code></pre> <p></p> <ul> <li>Get the project number</li> </ul> <pre><code>PROJECT_NUMBER=$(GCLOUD iam service-accounts list | grep EMAIL | cut -d':' -f2 | cut -d'-' -f1 | sed -e \"s/ //g\" | tr -d \"[\\n\\r]\")\n</code></pre> <p></p> <ul> <li>Set the  Identity and Access Management (IAM) permissions. (See Access control overview and Service accounts for more details)</li> </ul> <p><pre><code>$GCLOUD services enable cloudbuild.googleapis.com\n</code></pre> <pre><code>$GCLOUD projects add-iam-policy-binding $PROJECT \\\n   --member serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\n   --role roles/compute.admin\n</code></pre> <pre><code>$GCLOUD projects add-iam-policy-binding $PROJECT \\\n   --member serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\n   --role roles/iam.serviceAccountUser\n</code></pre> <pre><code>$GCLOUD projects add-iam-policy-binding $PROJECT \\\n   --member serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\n   --role roles/iam.serviceAccountTokenCreator\n</code></pre></p> <p></p>"},{"location":"google-cloud/#import-the-vmdk-file","title":"Import the VMDK File","text":"<ul> <li>Uses batch Windows command instead of cygwin bash. Indeed Python been an Anaconda tool for Windows,  it does not correctly deal with files based on cygwin pathways, even if the pathway is simply the  file name. Very strange! With this tip, it works. Perhaps we should have used the module for Anaconda instead (see google-cloud-sdk for Anaconda).</li> </ul> <pre><code>(\ncd $VDISK_DIR\n\necho \"@ECHO OFF\nCLS\nSET PATH=C:\\_Tools\\gcloud\\google-cloud-sdk\\bin;%PATH%;\ngcloud compute images import $IMAGE --zone $ZONE --no-guest-environment --source-file $VMDK_FILE --os $VM_OS \nexit /b 0\n\" &gt; /tmp/cmd.bat\nchmod +x /tmp/cmd.bat\n/tmp/cmd.bat\n)\n</code></pre> <ul> <li>Output generated <pre><code>Copying [box-disk001.vmdk] to [gs://test-gaev-20210906-daisy-bkt/tmpimage/5044ee90-6a16-43dd-9eff-789bf333c82c-box-disk001.vmdk]...\n..............................................................................................done.\nWARNING: Importing image. This may take up to 2 hours.\nCreated [https://cloudbuild.googleapis.com/v1/projects/test-gaev-20210906/builds/c1a34fc5-d8d7-4804-be22-58508bef1f82].\nLogs are available at [https://console.cloud.google.com/cloud-build/builds/c1a34fc5-d8d7-4804-be22-58508bef1f82?project=188926165780].\nstarting build \"c1a34fc5-d8d7-4804-be22-58508bef1f82\"\n[import-image]: 2021-09-07T14:30:25Z Creating Google Compute Engine disk from gs://test-gaev-20210906-daisy-bkt/tmpimage/5044ee90-6a16-43dd-9eff-789bf333c82c-box-disk001.vmdk\n[import-image]: 2021-09-07T14:38:23Z Finished creating Google Compute Engine disk\n[import-image]: 2021-09-07T14:38:23Z Inspecting disk for OS and bootloader\n[import-image]: 2021-09-07T14:40:17Z Inspection result=os_release:{cli_formatted:\"ubuntu-1804\" distro:\"ubuntu\" major_version:\"18\" minor_version:\"04\" architecture:X64 distro_id:UBUNTU} elapsed_time_ms:114577 os_count:1\n[import-image]: 2021-09-07T14:40:17Z Cloud Build ID:\n[import-image]: 2021-09-07T14:40:35Z Making disk bootable on Google Compute Engine\n</code></pre></li> </ul> Checking in the GCP console <ul> <li>List the images linked to the project</li> </ul> <pre><code>$GCLOUD compute images list | grep -B1 -A3 $PROJECT\n</code></pre> <ul> <li>Output generated <pre><code>NAME: jupyterhub-img\nPROJECT: test-gaev-20210906\nFAMILY:\nDEPRECATED:\nSTATUS: READY\n</code></pre></li> </ul> Checking in the GCP console"},{"location":"google-cloud/#create-an-instance-based-on-an-imported-image","title":"Create an instance based on an imported image","text":"<ul> <li>Launch the VM instance creation.  The type of virtual machines (VM_MACHINE) can be adapted according to your needs (see machine families)</li> </ul> <pre><code>$GCLOUD compute instances create $VM --image-project $PROJECT --image $IMAGE \\\n       --tags=http-server,https-server --zone=$ZONE --machine-type $VM_MACHINE\n</code></pre> <ul> <li>Output generated <pre><code>Created [https://www.googleapis.com/compute/v1/projects/test-gaev-20210906/zones/europe-west1-b/instances/jupyterhub-vm].\nNAME: jupyterhub-vm\nZONE: europe-west1-b\nMACHINE_TYPE: c2-standard-4\nPREEMPTIBLE:\nINTERNAL_IP: 10.132.0.25\nEXTERNAL_IP: 35.187.120.148\nSTATUS: RUNNING\n</code></pre></li> </ul> Checking in the GCP console <ul> <li>Get the IP address of the instance</li> </ul> <pre><code>IP=$($GCLOUD compute instances describe $VM \\\n   --format='get(networkInterfaces[0].accessConfigs[0].natIP)' | tr -d \"\\n\" | tr -d \"\\r\")\n</code></pre> <ul> <li>Remove previous ssh keys for this IP in the known_hosts file</li> </ul> <p><pre><code>grep -E -v \"^$IP\" ~/.ssh/known_hosts &gt; ~/.ssh/known_hosts.tmp\ncat ~/.ssh/known_hosts.tmp &gt; ~/.ssh/known_hosts\nssh -o 'StrictHostKeyChecking no' root@$IP \"hostname\"\n</code></pre> * Output generated <pre><code>Warning: Permanently added '35.187.120.148' (ED25519) to the list of known hosts.\njupyterhub-vm\n</code></pre></p> <p></p> <ul> <li>Customization : Fixe the issue concerning the external IP address </li> </ul> <p>Because the IP address found by default is the internal one we need the external IP address. So we have to modify the get-hostname script for that.</p> <pre><code>ssh root@$IP \"echo \\\"echo $IP\\\" &gt; /usr/local/bin/get-hostname\"\n</code></pre> <ul> <li>Then reboot the VM</li> </ul> <pre><code>ssh root@$IP \"reboot\"\n</code></pre> <p></p> <ul> <li>Launch Jupyterhub in your web browser</li> </ul> <p></p> <p></p> <ul> <li>Additional document: Slides that show, step by step, how to proceed from Google Cloud Console.</li> </ul> <p></p>"},{"location":"os-cloud/","title":"Openstack Cloud","text":"<p>GenOuest is a national bioinformatics platform federated by the French Institute of Bioinformatics (IFB). This platform offers cloud services for the French public research community. Any researcher from this community can make a request to Genouest or any other IFB platform to have access to the proposed services.</p> <p>GenOuest offers on its datacenter infrastructure a service of providing computing resources in the form of virtual machines. The infrastructure is managed using Openstack, which is a set of open source software that allows the deployment of cloud computing infrastructures (Infrastructure as a Service, IaS). It is therefore necessary to have a valid account on this infrastructure (see the online help).</p> <p>It is then assumed that you have installed Python (\u22652.7) as well as the package python-openstackclient . </p> <p></p>"},{"location":"os-cloud/#retrieving-its-connection-settings-from-genouest-openstack","title":"Retrieving its connection settings from GenOuest Openstack","text":"<p>To set the required environment variables for openstack command line clients, you must download the environment file called openrc.sh from the GenOuest openStack dashboard as a user. This project-specific environment file contains the credentials that all openstack services use.</p> <p></p> <p></p> <p>Note: All the scripts and other configuration files mentioned in this description can be found in free access under the GiHub INRAE repository.</p> <p></p> <p>You need to get the openrc.sh script as well as the cloud.yml configuration file. The latter is to be put in the &lt;home directory&gt;/.config/openstack directory.</p> <p>Then run the openrc.sh script in the current shell (i.e invoking using a dot); you will be asked for your password.</p> <pre><code>$&gt; . ./openrc.sh\nPlease enter your OpenStack Password for project &lt;Project&gt; as user &lt;Users&gt;:\n</code></pre> <ul> <li>&lt;Project&gt; and &lt;User&gt; corresponding to your configuration.</li> </ul> <p>This script will define the following variables: OS_AUTH_URL, OS_PROJECT_DOMAIN_ID, OS_REGION_NAME, OS_PROJECT_NAME, OS_USER_DOMAIN_NAME, OS_IDENTITY_API_VERSION, OS_INTERFACE, OS_USERNAME, OS_PROJECT_ID, OS_PASSWORD</p> <p>In addition, in order to simplify the written of commands, we will define some aliases.</p> <pre><code>OS_SCRIPTS=&lt;path of the python-openstackclient scripts&gt;\n\nalias ostack=\"$OS_SCRIPTS/openstack --os-cloud=openstack --os-password $OS_PASSWORD\"\nalias onova=\"$OS_SCRIPTS/nova --os-password $OS_PASSWORD\"\n</code></pre> <p></p> <p>If everything is configured correctly, the following command should provide you with a list of available flavors. Flavours define the hardware configuration available for a server. It defines the size of a virtual server that can be started.</p> <p><pre><code>$&gt; ostack flavor list\n</code></pre> <pre><code>+------------+-------+------+-------+-----------+\n | Name       |   RAM | Disk | VCPUs | Is Public |\n +------------+-------+------+-------+-----------+\n | m2.xlarge  | 16384 |   20 |     4 | True      |\n | m1.small   |  2048 |   20 |     1 | True      |\n | m2.large   |  8192 |   20 |     2 | True      |\n | m1.medium  |  4096 |   20 |     2 | True      |\n | m2.4xlarge | 65536 |   20 |     8 | True      |\n | m2.medium  |  4096 |   20 |     1 | True      |\n | m1.2xlarge | 32768 |   20 |     8 | True      |\n | m1.large   |  8192 |   20 |     4 | True      |\n | m1.xlarge  | 16384 |   20 |     8 | True      |\n | m2.2xlarge | 32768 |   20 |     4 | True      |\n +------------+-------+------+-------+-----------+\n</code></pre></p> <p></p> <p>We will then proceed in two steps:</p> <ol> <li>create a VM image within the library</li> <li>create an instance of this image</li> </ol> <p></p> <p></p>"},{"location":"os-cloud/#creating-the-image-on-the-openstack-infrastructure","title":"Creating the image on the openstack infrastructure","text":"<p>It is first necessary to extract the VM file in VMDK format from the archive generated in the previous step.</p> <p><pre><code>$&gt; tar xvzf &lt;path of the archive file&gt;/ubuntu-box.tar.gz box-disk001.vmdk\n</code></pre> This is the file that will be used to create the image. You must also specify a name for the image. </p> <p>Then the command below will create the image from the VM file. This command can take several minutes to be achieved.</p> <pre><code>IMAGE_NAME=jupyterhub-image\n\nostack image create --disk-format vmdk --file box-disk001.vmdk $IMAGE_NAME\n\nostack image set --property description='JupyterHub with R and Python' $IMAGE_NAME\n\nostack image show $IMAGE_NAME\n</code></pre> <p>As a result you should obtained something like below: <pre><code>+------------------+----------------------------------------------------------+\n| Field            | Value                                                    |\n+------------------+----------------------------------------------------------+\n| checksum         | 2a8e9789322009839401be1c25b3d977                         |\n| container_format | bare                                                     |\n| created_at       | 2021-09-09T06:49:14Z                                     |\n| disk_format      | vmdk                                                     |\n| file             | /v2/images/d88e2d16-9c67-4f07-a5dd-20d68f80ba0f/file     |\n| id               | d88e2d16-9c67-4f07-a5dd-20d68f80ba0f                     |\n| min_disk         | 0                                                        |\n| min_ram          | 0                                                        |\n| name             | jupyterhub-img                                           |\n| owner            | xxxxxxxxxxxxxxxxxxxxxxxxxx                               |\n| properties       | description='JupyterHub with R and Python'               |\n| protected        | False                                                    |\n| schema           | /v2/schemas/image                                        |\n| size             | 3666548736                                               |\n| status           | active                                                   |\n| tags             |                                                          |\n| updated_at       | 2021-09-09T07:02:16Z                                     |\n| visibility       | shared                                                   |\n+------------------+----------------------------------------------------------+\n</code></pre></p> <p></p>"},{"location":"os-cloud/#creating-an-instance-from-the-created-image","title":"Creating an instance from the created image","text":"<p>From the created image, it is now possible to create an instance. You have to specify the name of the instance, its flavor (see above) and the SSH key associated to this VM. The SSH key (keypair) must first have been created from the openstack dashboard.</p> <ul> <li> <p>You can list the SSH keys with the command: <pre><code>ostack keypair list\n</code></pre></p> </li> <li> <p>An SSH key has been created with the name genostack in the openstack dashboard. This SSH key must be a valid key on your (linux) openstack.genouest.org account(file ~/.ssh/authorized_keys).</p> </li> <li> <p>For flavor we choose in our example the one corresponding to 8 CPUs, 16GB of RAM and 20GB of disk.</p> </li> <li> <p>You can also specify the file containing the commands to be executed after the first boot. Here this file is necessary (user-data-jupystack.txt) because we must overwrite the file /usr/local/bin/get-hostname giving the complete name of the instance on openstack in order to build the root URL of the jupyterhub application (cf jupyterhub.pre and jupyterhub.service).</p> </li> </ul> <p></p> <p>Now, we are ready to launch the command to create the VM isntance.</p> <pre><code>IMAGE_NAME=jupyterhub-image\nSERVER_NAME=jupystack\nKEYPAIR=genostack\nFLAVOR_NAME=m1.xlarge\n\nIMAGEID=$(ostack image show $IMAGE_NAME | \\\n           grep \"| id \" | cut -d'|' -f3 | \\ \n           sed -e \"s/ //g\")\nFLAVORID=$(ostack flavor list | \\\n           grep \"$FLAVOR_NAME\"  | cut -d'|' -f2 | \\\n           sed -e \"s/ //g\")\n\nonova boot --flavor $FLAVORID --image $IMAGEID --security-groups default \\\n           --user-data ./user-data-jupystack.txt \\\n           --key-name $KEYPAIR  $SERVER_NAME\n</code></pre> <p>Once launched, the construction of the VM instance is underway, and its status is \"building\". So you have to wait until the VM status is \"Running\" (~ one minute or more).</p> <p>To see the status, launch the command below :</p> <pre><code>ostack server show $SERVER_NAME\n</code></pre> <p>You should probably rereun this last command</p> <p>Finally you should obtained something like below:</p> <pre><code>+-----------------------------+---------------------------------------------------------+\n| Field                       | Value                                                   |\n+-----------------------------+---------------------------------------------------------+\n| OS-DCF:diskConfig           | MANUAL                                                  |\n| OS-EXT-AZ:availability_zone | nova                                                    |\n| OS-EXT-STS:power_state      | Running                                                 |\n| OS-EXT-STS:task_state       | None                                                    |\n| OS-EXT-STS:vm_state         | active                                                  |\n| OS-SRV-USG:launched_at      | 2021-09-09T07:09:49.000000                              |\n| OS-SRV-USG:terminated_at    | None                                                    |\n| accessIPv4                  |                                                         |\n| accessIPv6                  |                                                         |\n| addresses                   | genouest-ext=192.168.101.79                             |\n| config_drive                | True                                                    |\n| created                     | 2021-09-09T07:08:05Z                                    |\n| flavor                      | m1.xlarge (c781b97e-f891-4ddf-9a6b-0e5ae4ef3eb1)        |\n| hostId                      | 126a5d6324eb3765547b4d2e3c5743509afde064a7d90ff1db2fb58a|\n| id                          | 0146be96-0c4f-413b-b3a5-bde37cdabb26                    |\n| image                       | jupyterhub-image (d88e2d16-9c67-4f07-a5dd-20d68f80ba0f) |\n| key_name                    | genostack                                               |\n| name                        | jupystack                                               |\n| progress                    | 0                                                       |\n| project_id                  | xxxxxxxxxxxxxxxxxxxxxxxxx                               |\n| properties                  |                                                         |\n| security_groups             | name='default'                                          |\n| status                      | ACTIVE                                                  |\n| updated                     | 2021-09-09T07:09:49Z                                    |\n| user_id                     | xxxxxxxxxxxxxxxxxxxxxxxxx                               |\n| volumes_attached            |                                                         |\n+-----------------------------+---------------------------------------------------------+\n</code></pre> <p></p> <p>The last command is also used to obtain the IP number of the VM thus created. The IP addresse is given by the field 'addresses'. Let's suppose that this IP number is 192.168.101.79.  You can then access the application from your web browser at the URL: https://app-192-168-101-79.vm.openstack.genouest.org/hub/login</p> <p></p> <p></p>"},{"location":"os-cloud/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank the IFB GenOuest bioinformatics for providing storage and computing resources on its national life science Cloud.</p> <ul> <li>Additional document: Slides that show, step by step, how to copy data, scripts and notebooks to the shared folder of the Jupyterhub server, in a secure way (SCP) using PuTTY and WinSCP.</li> </ul> <p></p>"}]}